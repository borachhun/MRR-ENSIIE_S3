---
title: "Practical session - Modèles de régression linéaire"
author: "Piseth KHENG, Borachhun YOU"
date: "19 September 2022"
output: pdf_document
---

# IV. Application: GAFAM or BATX data set

```{r, echo=FALSE}
rm(list=ls()); graphics.off()
```

The data set^[Source: https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/] below shows the number of monthly active users (MAU) on Facebook from 2008 to 2021 in millions. The numbers were taken from Q4 of each year except for the year 2008, whose data is only available in Q3.

```{r}
tab <- read.table("fb_mau.txt", header=TRUE, sep=",")
tab
```

The dimension of the data set:

```{r}
dim(tab)
```

We then try visualizing the data in order to see if there is an apparent linear relationship between the year and the number of users.

```{r}
plot(tab, xlab="Year", ylab="Monthly active users (MAU)",
     main="Facebook MAU from 2008 to 2021 (in millions)",
     pch=19, col=rgb(0.27,0.4,0.68))
```

Based on the graph above, we can see that the relationship is fairly linear. Therefore, we can use a linear model to represent the relationship.

```{r}
modreg = lm(mau ~ year, data=tab)
summary(modreg)
```

According to the summary of the model, the estimated intercept equals $-4.330 \times 10^5$ and the estimated coefficient of the year variable equals $2.157 \times 10^2$. The model can be written in the form:

$$\hat{y} = (-4.330 \times 10^5) + (2.157 \times 10^2) x + \hat{\epsilon}$$

where $x$ is the year variable, $\hat{y}$ is the prediction of the MAU and $\hat{\epsilon}$ is the residual.

As for the $R^2$, we can see that $R^2 = 0.9979 \approx 1$. It is a great result since the value corresponds to the Cosinus of the angle between the vector of the predicted value and the vector of the observed value, and the closer to 0 the angle gets, the better the model becomes.

```{r}
Y <- tab$mau
Y_predict <- predict(modreg, tab)

plot(Y, Y_predict, main="Observed and predicted values", pch=19, col=rgb(0.27,0.4,0.68))
grid()
abline(a=0, b=1, col="red")
```

In the graph $(y, \hat{y})$ above, we can see that the plotted points are fairly close to the bisector, which indicates that the model is acceptable.

The graph below is a scatter plot of the residuals for every pair of $(y, \hat{y})$.

```{r}
epsilon <- Y - Y_predict

plot(epsilon, main="Residuals", pch=19, col="red")
grid()
abline(a=0, b=0, col="black")
```

# V. Medical data

```{r, echo=FALSE}
rm(list=ls()); graphics.off()
```

```{r}
tab <- read.table("diabetes.txt", header=TRUE, sep="\t")
```

The dimension of the data set:

```{r}
dim(tab)
```

The names of the variables:

```{r}
names(tab)
```

The data set consists of $p=10$ co-variables and one target variable ("Y"), and $n=442$ observations.

We now try creating a linear model using the data set.

```{r}
modreg = lm(Y~., data=tab)
summary(modreg)
```

With the `summary()` function, we can see that the variable "SEX", "BMI", "BP" and "S5" are the most significant variables of the model. We can also see that $R^2 = 0.5177$ which is quite far from 1, indicating that the current linear regression is not doing well.

Using the obtained linear model, we further predict the values of $y$ and compare them with the observed values in the data set.

```{r}
Y <- tab$Y
Y_predict <- predict(modreg, tab)

plot(Y, Y_predict, col="blue", pch=20, main="Observed and predicted values")
grid()
abline(a=0, b=1, col="red")
```

Based on the $(y, \hat{y})$ graph above, we can see that the plotted points are scattered around the bisector, with some points pretty far away from the bisector, rather that appearing on the line. This shows that the differences between the predicted values and the observed values are quite significant and thus the linear model is not good enough.

The scatter plot below shows the residual for every pair of $(y, \hat{y})$.

```{r}
epsilon <- Y - Y_predict

plot(epsilon, col="red", pch=20, main="Residuals")
grid()
abline(a=0, b=0, col="black")
```